var documenterSearchIndex = {"docs":
[{"location":"PencilArrays/#PencilArrays_module","page":"Array wrappers","title":"Array wrappers","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"CurrentModule = PencilArrays","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"The PencilArrays module defines types for handling MPI-distributed data.","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"The most important types are:","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"PencilArray: array wrapper including MPI decomposition information. Takes local indices starting at 1, regardless of the location of each MPI process on the global topology.\nGlobalPencilArray: PencilArray wrapper that takes global indices, which generally don't start at 1. See also Global views.","category":"page"},{"location":"PencilArrays/#Construction","page":"Array wrappers","title":"Construction","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"An uninitialised PencilArray can be constructed from a Pencil instance as","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"pencil = Pencil(#= ... =#)\nA = PencilArray{Float64}(undef, pencil)\nparent(A)  # returns the Array wrapped by `A`","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"This allocates a new Array with the local dimensions and data type associated to the Pencil.","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"One can also construct a PencilArray wrapper from an existing AbstractArray, whose dimensions must be compatible with the Pencil configuration. For instance, the following works:","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"dims = size_local(pencil, MemoryOrder())  # dimensions must be in memory order!\ndata = zeros(dims)\nA = PencilArray(pencil, data)","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"Note that data does not need to be a Array, but can be any subtype of AbstractArray.","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"It is also possible to construct higher dimensional arrays, as in:","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"data = zeros(dims..., 3, 2)\nA = PencilArray(pencil, data)","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"This will construct a PencilArray where the rightmost dimensions (called extra dimensions in the PencilArrays API) will never be split among MPI processes.","category":"page"},{"location":"PencilArrays/#Dimension-permutations","page":"Array wrappers","title":"Dimension permutations","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"Unlike the wrapped AbstractArray, the PencilArray wrapper takes indices in logical order. For instance, if the underlying permutation of the Pencil is (2, 3, 1), then A[i, j, k] points to the same value as parent(A)[j, k, i].","category":"page"},{"location":"PencilArrays/#Global-views","page":"Array wrappers","title":"Global views","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"PencilArrays are accessed using local indices that start at 1, regardless of the location of the subdomain associated to the local process on the global grid. Sometimes it may be more convenient to use global indices describing the position of the local process in the domain. For this, the global_view function is provided that generates an OffsetArray wrapper taking global indices.","category":"page"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"For more details, see for instance the gradient example in the PencilFFTs docs.","category":"page"},{"location":"PencilArrays/#Types","page":"Array wrappers","title":"Types","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"PencilArray\nGlobalPencilArray\nPencilArrayCollection\nManyPencilArray","category":"page"},{"location":"PencilArrays/#PencilArrays.PencilArray","page":"Array wrappers","title":"PencilArrays.PencilArray","text":"PencilArray(pencil::Pencil, data::AbstractArray{T,N})\n\nCreate array wrapper with pencil decomposition information.\n\nThe array dimensions and element type must be consistent with those of the given pencil.\n\nnote: Index permutations\nIf the Pencil has an associated index permutation, then data must have its dimensions permuted accordingly (in memory order).Unlike data, the resulting PencilArray should be accessed with unpermuted indices (in logical order).ExampleSuppose pencil has local dimensions (10, 20, 30) before permutation, and has an asociated permutation (2, 3, 1). Then:data = zeros(20, 30, 10)       # parent array (with dimensions in memory order)\n\nu = PencilArray(pencil, data)  # wrapper with dimensions (10, 20, 30)\n@assert size(u) === (10, 20, 30)\n\nu[15, 25, 5]          # BoundsError (15 > 10 and 25 > 20)\nu[5, 15, 25]          # correct\nparent(u)[15, 25, 5]  # correct\n\n\nnote: Extra dimensions\nThe data array can have one or more extra dimensions to the right (slow indices), which are not affected by index permutations.Exampledims = (20, 30, 10)\nPencilArray(pencil, zeros(dims...))        # works (scalar)\nPencilArray(pencil, zeros(dims..., 3))     # works (3-component vector)\nPencilArray(pencil, zeros(dims..., 4, 3))  # works (4×3 tensor)\nPencilArray(pencil, zeros(3, dims...))     # fails\n\n\n\nPencilArray{T}(undef, pencil::Pencil, [extra_dims...])\n\nAllocate an uninitialised PencilArray that can hold data in the local pencil.\n\nExtra dimensions, for instance representing vector components, can be specified. These dimensions are added to the rightmost (slowest) indices of the resulting array.\n\nExample\n\nSuppose pencil has local dimensions (20, 10, 30). Then:\n\nPencilArray{Float64}(undef, pencil)        # array dimensions are (20, 10, 30)\nPencilArray{Float64}(undef, pencil, 4, 3)  # array dimensions are (20, 10, 30, 4, 3)\n\nMore examples:\n\njulia> pen = Pencil((20, 10, 12), MPI.COMM_WORLD);\n\njulia> u = PencilArray{Float64}(undef, pen);\n\njulia> summary(u)\n\"20×10×12 PencilArray{Float64, 3}(::Pencil{3, 2, NoPermutation})\"\n\njulia> PencilArray{Float64}(undef, pen, 4, 3) |> summary\n\"20×10×12×4×3 PencilArray{Float64, 5}(::Pencil{3, 2, NoPermutation})\"\n\n\n\n\n\n\n","category":"type"},{"location":"PencilArrays/#PencilArrays.GlobalPencilArray","page":"Array wrappers","title":"PencilArrays.GlobalPencilArray","text":"GlobalPencilArray{T,N} <: AbstractArray{T,N}\n\nAlias for an OffsetArray wrapping a PencilArray.\n\nUnlike PencilArrays, GlobalPencilArrays take global indices, which in general don't start at 1 for a given MPI process.\n\nThe global_view function should be used to create a GlobalPencilArray from a PencilArray.\n\n\n\n\n\n","category":"type"},{"location":"PencilArrays/#PencilArrays.PencilArrayCollection","page":"Array wrappers","title":"PencilArrays.PencilArrayCollection","text":"PencilArrayCollection\n\nUnionAll type describing a collection of PencilArrays.\n\nSuch a collection can be a tuple or an array of PencilArrays.\n\nCollections are by assumption homogeneous: each array has the same properties, and in particular, is associated to the same Pencil configuration.\n\nFor convenience, certain operations defined for PencilArray are also defined for PencilArrayCollection, and return the same value as for a single PencilArray. Some examples are pencil, range_local and get_comm.\n\nAlso note that functions from Base, such as size, ndims and eltype, are not overloaded for PencilArrayCollection, since they already have a definition for tuples and arrays (and redefining them would be type piracy...).\n\n\n\n\n\n","category":"type"},{"location":"PencilArrays/#PencilArrays.ManyPencilArray","page":"Array wrappers","title":"PencilArrays.ManyPencilArray","text":"ManyPencilArray{T,N,M}\n\nContainer holding M different PencilArray views to the same underlying data buffer. All views share the same element type T and dimensionality N.\n\nThis can be used to perform in-place data transpositions with transpose!.\n\n\n\nManyPencilArray{T}(undef, pencils...; extra_dims=())\n\nCreate a ManyPencilArray container that can hold data of type T associated to all the given Pencils.\n\nThe optional extra_dims argument is the same as for PencilArray.\n\n\n\n\n\n","category":"type"},{"location":"PencilArrays/#Methods","page":"Array wrappers","title":"Methods","text":"","category":"section"},{"location":"PencilArrays/#PencilArray","page":"Array wrappers","title":"PencilArray","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"extra_dims(::PencilArray)\nget_comm(::MaybePencilArrayCollection)\npermutation\nglobal_view(::PencilArray)\nndims_extra(::MaybePencilArrayCollection)\nndims_space(::PencilArray)\nparent(::PencilArray)\npencil(::PencilArray)\npointer(::PencilArray)\nrange_local(::MaybePencilArrayCollection)\nrange_remote(::MaybePencilArrayCollection, etc...)\nsimilar(::PencilArray)\nsize(::PencilArray)\nsize_local(::MaybePencilArrayCollection)\nsize_global(::MaybePencilArrayCollection)\nsizeof_global(::PencilArray)\ntopology(::MaybePencilArrayCollection)","category":"page"},{"location":"PencilArrays/#PencilArrays.extra_dims-Tuple{PencilArray}","page":"Array wrappers","title":"PencilArrays.extra_dims","text":"extra_dims(x::PencilArray)\nextra_dims(x::PencilArrayCollection)\n\nReturn tuple with size of \"extra\" dimensions of PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.MPITopologies.get_comm-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"Array wrappers","title":"PencilArrays.Pencils.MPITopologies.get_comm","text":"get_comm(x::PencilArray)\nget_comm(x::PencilArrayCollection)\n\nGet MPI communicator associated to a pencil-distributed array.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.permutation","page":"Array wrappers","title":"PencilArrays.Pencils.permutation","text":"permutation(::Type{<:PencilArray})\npermutation(x::PencilArray)\npermutation(x::PencilArrayCollection)\n\nGet index permutation associated to the given PencilArray.\n\nReturns NoPermutation() if there is no associated permutation.\n\n\n\n\n\n","category":"function"},{"location":"PencilArrays/#PencilArrays.global_view-Tuple{PencilArray}","page":"Array wrappers","title":"PencilArrays.global_view","text":"global_view(x::PencilArray)\n\nCreate an OffsetArray of a PencilArray that takes global indices in logical order.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.ndims_extra-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"Array wrappers","title":"PencilArrays.ndims_extra","text":"ndims_extra(::Type{<:PencilArray})\nndims_extra(x::PencilArray)\nndims_extra(x::PencilArrayCollection)\n\nNumber of \"extra\" dimensions associated to PencilArray.\n\nThese are the dimensions that are not associated to the domain geometry. For instance, they may correspond to vector or tensor components.\n\nThese dimensions correspond to the rightmost indices of the array.\n\nThe total number of dimensions of a PencilArray is given by:\n\nndims(x) == ndims_space(x) + ndims_extra(x)\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.ndims_space-Tuple{PencilArray}","page":"Array wrappers","title":"PencilArrays.ndims_space","text":"ndims_space(x::PencilArray)\nndims_space(x::PencilArrayCollection)\n\nNumber of dimensions associated to the domain geometry.\n\nThese dimensions correspond to the leftmost indices of the array.\n\nThe total number of dimensions of a PencilArray is given by:\n\nndims(x) == ndims_space(x) + ndims_extra(x)\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.parent-Tuple{PencilArray}","page":"Array wrappers","title":"Base.parent","text":"parent(x::PencilArray)\n\nReturn array wrapped by a PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.pencil-Tuple{PencilArray}","page":"Array wrappers","title":"PencilArrays.pencil","text":"pencil(x::PencilArray)\n\nReturn decomposition configuration associated to a PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.pointer-Tuple{PencilArray}","page":"Array wrappers","title":"Base.pointer","text":"pointer(x::PencilArray)\n\nReturn pointer to the start of the underlying data.\n\nUse with caution: this may not make a lot of sense if the underlying data is not contiguous or strided (e.g. if the PencilArray is wrapping a non-strided SubArray).\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.range_local-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"Array wrappers","title":"PencilArrays.Pencils.range_local","text":"range_local(x::PencilArray, [order = LogicalOrder()])\nrange_local(x::PencilArrayCollection, [order = LogicalOrder()])\n\nLocal data range held by the PencilArray.\n\nBy default the dimensions are returned in logical order.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.range_remote-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray},Vararg{Any,N} where N}","page":"Array wrappers","title":"PencilArrays.Pencils.range_remote","text":"range_remote(x::PencilArray, coords, [order = LogicalOrder()])\nrange_remote(x::PencilArrayCollection, coords, [order = LogicalOrder()])\n\nGet data range held by the PencilArray in a given MPI process.\n\nThe location of the MPI process in the topology is determined by the coords argument, which can be given as a linear or Cartesian index.\n\nSee range_remote(::Pencil, ...) variant for details.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.similar-Tuple{PencilArray}","page":"Array wrappers","title":"Base.similar","text":"similar(x::PencilArray, [element_type=eltype(x)], [dims])\n\nReturns an array similar to x.\n\nThe actual type of the returned array depends on whether dims is passed:\n\nif dims is not passed, then a PencilArray of same dimensions of x is returned.\notherwise, an array similar to that wrapped by x (typically a regular Array) is returned, with the chosen dimensions.\n\nExamples\n\njulia> pen = Pencil((20, 10, 12), MPI.COMM_WORLD);\n\njulia> u = PencilArray{Float64}(undef, pen);\n\njulia> similar(u) |> summary\n\"20×10×12 PencilArray{Float64, 3}(::Pencil{3, 2, NoPermutation})\"\n\njulia> similar(u, ComplexF32) |> summary\n\"20×10×12 PencilArray{ComplexF32, 3}(::Pencil{3, 2, NoPermutation})\"\n\njulia> similar(u, (4, 3, 8)) |> summary\n\"4×3×8 Array{Float64, 3}\"\n\njulia> similar(u, (4, 3)) |> summary\n\"4×3 Matrix{Float64}\"\n\njulia> similar(u, ComplexF32) |> summary\n\"20×10×12 PencilArray{ComplexF32, 3}(::Pencil{3, 2, NoPermutation})\"\n\njulia> similar(u, ComplexF32, (4, 3)) |> summary\n\"4×3 Matrix{ComplexF32}\"\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.size-Tuple{PencilArray}","page":"Array wrappers","title":"Base.size","text":"size(x::PencilArray)\n\nReturn local dimensions of a PencilArray in logical order.\n\nSame as size_local(x, LogicalOrder()) (see size_local).\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.size_local-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"Array wrappers","title":"PencilArrays.Pencils.size_local","text":"size_local(x::PencilArray, [order = LogicalOrder()])\nsize_local(x::PencilArrayCollection, [order = LogicalOrder()])\n\nLocal dimensions of the data held by the PencilArray.\n\nIf order = LogicalOrder(), this is the same as size(x).\n\nSee also size_local(::Pencil).\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.size_global-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"Array wrappers","title":"PencilArrays.Pencils.size_global","text":"size_global(x::PencilArray, [order = LogicalOrder()])\nsize_global(x::PencilArrayCollection, [order = LogicalOrder()])\n\nGlobal dimensions associated to the given array.\n\nBy default, the logical dimensions of the dataset are returned.\n\nSee also size_global(::Pencil).\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.sizeof_global-Tuple{PencilArray}","page":"Array wrappers","title":"PencilArrays.sizeof_global","text":"sizeof_global(x::PencilArray)\nsizeof_global(x::PencilArrayCollection)\n\nGlobal size of array in bytes.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#PencilArrays.Pencils.topology-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"Array wrappers","title":"PencilArrays.Pencils.topology","text":"topology(x::PencilArray)\ntopology(x::PencilArrayCollection)\n\nGet MPITopology associated to a PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#ManyPencilArray","page":"Array wrappers","title":"ManyPencilArray","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"first(::ManyPencilArray)\ngetindex(::ManyPencilArray)\nlast(::ManyPencilArray)\nlength(::ManyPencilArray)","category":"page"},{"location":"PencilArrays/#Base.first-Tuple{ManyPencilArray}","page":"Array wrappers","title":"Base.first","text":"first(A::ManyPencilArray)\n\nReturns the first PencilArray wrapped by A.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.getindex-Tuple{ManyPencilArray}","page":"Array wrappers","title":"Base.getindex","text":"getindex(A::ManyPencilArray, ::Val{i})\ngetindex(A::ManyPencilArray, i::Integer)\n\nReturns the i-th PencilArray wrapped by A.\n\nIf possible, the Val{i} form should be preferred, as it is more efficient and it allows the compiler to know the return type.\n\nSee also first(::ManyPencilArray), last(::ManyPencilArray).\n\nExample\n\nA = ManyPencilArray(pencil1, pencil2, pencil3)\n\n# Get the PencilArray associated to `pencil2`.\n# u2 = A[2]\nu2 = A[Val(2)]  # faster!\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.last-Tuple{ManyPencilArray}","page":"Array wrappers","title":"Base.last","text":"last(A::ManyPencilArray)\n\nReturns the last PencilArray wrapped by A.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Base.length-Tuple{ManyPencilArray}","page":"Array wrappers","title":"Base.length","text":"length(A::ManyPencilArray)\n\nReturns the number of PencilArrays wrapped by A.\n\n\n\n\n\n","category":"method"},{"location":"PencilArrays/#Index","page":"Array wrappers","title":"Index","text":"","category":"section"},{"location":"PencilArrays/","page":"Array wrappers","title":"Array wrappers","text":"Pages = [\"PencilArrays.md\"]","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"CurrentModule = PencilArrays.Pencils","category":"page"},{"location":"Pencils/#sec:pencil_configs","page":"Pencil configurations","title":"Pencil configurations","text":"","category":"section"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"A pencil configuration refers to a given distribution of multidimensional data among MPI processes. This information is encoded in the Pencil type.","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"A pencil configuration includes:","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"MPI topology information,\nglobal and local dimensions of the numerical grid,\nsubset of decomposed dimensions,\ndefinition of optional permutation of dimensions.","category":"page"},{"location":"Pencils/#Construction","page":"Pencil configurations","title":"Construction","text":"","category":"section"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"The creation of a new Pencil requires a MPITopology and the global data dimensions. One may also specify the list of decomposed dimensions, as well as an optional permutation of dimensions.","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"For instance, to decompose along 2 dimensions of a 3D dataset,","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"topology = MPITopology(comm, (8, 4))  # assuming 8×4 = 32 processes\ndims_global = (16, 32, 64)\npencil = Pencil(topology, dims_global)","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"By default, the decomposed dimensions are the rightmost ones (in this case, dimensions 2 and 3). A different set of dimensions may be selected via the optional positional argument. For instance, to decompose along dimensions 1 and 3 instead,","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"decomp_dims = (1, 3)\npencil = Pencil(topology, dims_global, decomp_dims)","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"One may also want to work with multiple pencil configurations that differ, for instance, on the selection of decomposed dimensions. For this case, a second constructor is available that takes an already existing Pencil instance. Calling this constructor should be preferred when possible since it allows sharing memory buffers (used for instance for global transpositions) and thus reducing memory usage. The following creates a Pencil equivalent to the one above, but with different decomposed dimensions:","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"pencil_x = Pencil(pencil; decomp_dims=(1, 2))","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"See the Pencil documentation for more details.","category":"page"},{"location":"Pencils/#Dimension-permutations","page":"Pencil configurations","title":"Dimension permutations","text":"","category":"section"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"As mentioned above, a Pencil may optionally be given information on dimension permutations. In this case, the layout of the data arrays in memory is different from the logical order of dimensions. For performance reasons, permutations are compile-time objects defined in the StaticPermutations package.","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"To make permutations clearer, consider the example above where the global data dimensions are N_x  N_y  N_z = 16  32  64. In this case, the logical order is (x y z). Now let's say that we want the memory order of the data to be (y z x),[1] which corresponds to the permutation (2, 3, 1).","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"Permutations are passed to the Pencil constructor via the permute keyword argument. Dimension permutations should be specified using a Permutation object. For instance,","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"permutation = Permutation(2, 3, 1)\npencil = Pencil(#= ... =#, permute=permutation)","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"One can also pass a NoPermutation to disable permutations (this is the default).","category":"page"},{"location":"Pencils/#Types","page":"Pencil configurations","title":"Types","text":"","category":"section"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"Pencil\nPencils.AbstractIndexOrder\nMemoryOrder\nLogicalOrder","category":"page"},{"location":"Pencils/#PencilArrays.Pencils.Pencil","page":"Pencil configurations","title":"PencilArrays.Pencils.Pencil","text":"Pencil{N,M}\n\nDescribes the decomposition of an N-dimensional array among MPI processes along M directions (with M < N).\n\n\n\nPencil(\n    topology::MPITopology{M}, size_global::Dims{N},\n    decomp_dims::Dims{M} = default_decomposition(N, Val(M));\n    permute::AbstractPermutation = NoPermutation(),\n    timer = TimerOutput(),\n)\n\nDefine the decomposition of an N-dimensional geometry along M dimensions.\n\nThe dimensions of the geometry are given by size_global = (N1, N2, ...). The Pencil describes the decomposition of an array of dimensions size_global across a group of MPI processes.\n\nData is distributed over the given M-dimensional MPI topology (with M < N).\n\nThe decomposed dimensions may optionally be provided via the decomp_dims argument. By default, the M rightmost dimensions are decomposed. For instance, for a 2D decomposition of 5D data (M = 2 and N = 5), the dimensions (4, 5) are decomposed by default.\n\nThe optional parameter perm should be a (compile-time) tuple defining a permutation of the data indices. Such permutation may be useful for performance reasons, since it may be preferable (e.g. for FFTs) that the data is contiguous along the pencil direction.\n\nIt is also possible to pass a TimerOutput to the constructor. See Measuring performance for details.\n\nExamples\n\nDecompose a 3D geometry of global dimensions N_x  N_y  N_z = 4812 along the second (y) and third (z) dimensions:\n\njulia> topo = MPITopology(MPI.COMM_WORLD, Val(2));\n\njulia> Pencil(topo, (4, 8, 12), (2, 3))\nDecomposition of 3D data\n    Data dimensions: (4, 8, 12)\n    Decomposed dimensions: (2, 3)\n    Data permutation: NoPermutation()\n\njulia> Pencil(topo, (4, 8, 12), (2, 3); permute = Permutation(3, 2, 1))\nDecomposition of 3D data\n    Data dimensions: (4, 8, 12)\n    Decomposed dimensions: (2, 3)\n    Data permutation: Permutation(3, 2, 1)\n\n\nIn the second case, the actual data is stored in (z, y, x) order within each MPI process.\n\n\n\nPencil(size_global::Dims{N}, [decomp_dims = (2, …, N)], comm::MPI.Comm; kws...)\n\nConvenience constructor that implicitly creates a MPITopology.\n\nThe number of decomposed dimensions specified by decomp_dims must be M < N. If decomp_dims is not passed, dimensions 2:N are decomposed.\n\nKeyword arguments are passed to alternative constructor taking an MPITopology. That constructor should be used if more control is desired.\n\nExamples\n\njulia> Pencil((4, 8, 12), MPI.COMM_WORLD)\nDecomposition of 3D data\n    Data dimensions: (4, 8, 12)\n    Decomposed dimensions: (2, 3)\n    Data permutation: NoPermutation()\n\njulia> Pencil((4, 8, 12), (1, ), MPI.COMM_WORLD)\nDecomposition of 3D data\n    Data dimensions: (4, 8, 12)\n    Decomposed dimensions: (1,)\n    Data permutation: NoPermutation()\n\n\n\nPencil(\n    p::Pencil{N,M};\n    decomp_dims::Dims{M} = decomposition(p),\n    size_global::Dims{N} = size_global(p),\n    permute::P = permutation(p),\n    timer::TimerOutput = timer(p),\n)\n\nCreate new pencil configuration from an existent one.\n\nThis constructor enables sharing temporary data buffers between the two pencil configurations, leading to reduced global memory usage.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#PencilArrays.Pencils.AbstractIndexOrder","page":"Pencil configurations","title":"PencilArrays.Pencils.AbstractIndexOrder","text":"AbstractIndexOrder\n\nAbstract type determining the ordering of dimensions of an array with possibly permuted indices.\n\nSubtypes are MemoryOrder and LogicalOrder.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#PencilArrays.Pencils.MemoryOrder","page":"Pencil configurations","title":"PencilArrays.Pencils.MemoryOrder","text":"MemoryOrder <: AbstractIndexOrder\n\nSingleton type specifying that array dimensions should be given in memory (or permuted) order.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#PencilArrays.Pencils.LogicalOrder","page":"Pencil configurations","title":"PencilArrays.Pencils.LogicalOrder","text":"LogicalOrder <: AbstractIndexOrder\n\nSingleton type specifying that array dimensions should be given in logical (or non-permuted) order.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#Methods","page":"Pencil configurations","title":"Methods","text":"","category":"section"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"topology(::Pencil)\nget_comm(::Pencil)\ndecomposition(::Pencil)\npermutation(::Pencil)\ntimer(::Pencil)\nlength(::Pencil)\nndims(::Pencil)\nrange_remote(::Pencil, ::Integer, ::LogicalOrder)\nrange_local(::Pencil, ::LogicalOrder)\nsize_global(::Pencil, ::LogicalOrder)\nsize_local(::Pencil, etc...)\nto_local(::Pencil)","category":"page"},{"location":"Pencils/#PencilArrays.Pencils.topology-Tuple{Pencil}","page":"Pencil configurations","title":"PencilArrays.Pencils.topology","text":"topology(p::Pencil)\n\nGet MPITopology attached to Pencil.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.MPITopologies.get_comm-Tuple{Pencil}","page":"Pencil configurations","title":"PencilArrays.Pencils.MPITopologies.get_comm","text":"get_comm(p::Pencil)\n\nGet MPI communicator associated to an MPI decomposition scheme.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.decomposition-Tuple{Pencil}","page":"Pencil configurations","title":"PencilArrays.Pencils.decomposition","text":"decomposition(p::Pencil)\n\nGet tuple with decomposed dimensions of the given pencil configuration.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.permutation-Tuple{Pencil}","page":"Pencil configurations","title":"PencilArrays.Pencils.permutation","text":"permutation(::Type{<:Pencil}) -> AbstractPermutation\npermutation(p::Pencil)        -> AbstractPermutation\n\nGet index permutation associated to the given pencil configuration.\n\nReturns NoPermutation() if there is no associated permutation.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.timer-Tuple{Pencil}","page":"Pencil configurations","title":"PencilArrays.Pencils.timer","text":"timer(p::Pencil)\n\nGet TimerOutput attached to a Pencil.\n\nSee Measuring performance for details.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.length-Tuple{Pencil}","page":"Pencil configurations","title":"Base.length","text":"length(p::Pencil)\n\nGet linear length of data associated to the local pencil layout.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.ndims-Tuple{Pencil}","page":"Pencil configurations","title":"Base.ndims","text":"ndims(p::Pencil)\n\nNumber of spatial dimensions associated to pencil data.\n\nThis corresponds to the total number of dimensions of the space, which includes the decomposed and non-decomposed dimensions.\n\n\n\n\n\nndims(t::MPITopology)\n\nGet dimensionality of Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.range_remote-Tuple{Pencil,Integer,LogicalOrder}","page":"Pencil configurations","title":"PencilArrays.Pencils.range_remote","text":"range_remote(p::Pencil, coords, [order = LogicalOrder()])\nrange_remote(p::Pencil, n::Integer, [order = LogicalOrder()])\n\nGet data range held by a given MPI process.\n\nIn the first variant, coords are the coordinates of the MPI process in the Cartesian topology. They can be specified as a tuple (i, j, ...) or as a CartesianIndex.\n\nIn the second variant, n is the linear index of a given process in the topology.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.range_local-Tuple{Pencil,LogicalOrder}","page":"Pencil configurations","title":"PencilArrays.Pencils.range_local","text":"range_local(p::Pencil, [order = LogicalOrder()])\n\nLocal data range held by the pencil.\n\nBy default the dimensions are not permuted, i.e. they follow the logical order of dimensions.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.size_global-Tuple{Pencil,LogicalOrder}","page":"Pencil configurations","title":"PencilArrays.Pencils.size_global","text":"size_global(p::Pencil, [order = LogicalOrder()])\n\nGlobal dimensions of the Cartesian grid associated to the given domain decomposition.\n\nLike size_local, by default the returned dimensions are in logical order.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.size_local-Tuple{Pencil,Vararg{Any,N} where N}","page":"Pencil configurations","title":"PencilArrays.Pencils.size_local","text":"size_local(p::Pencil, [order = LogicalOrder()])\n\nLocal dimensions of the data held by the pencil.\n\nBy default the dimensions are not permuted, i.e. they follow the logical order of dimensions.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilArrays.Pencils.to_local-Tuple{Pencil}","page":"Pencil configurations","title":"PencilArrays.Pencils.to_local","text":"to_local(p::Pencil, global_inds, [order = LogicalOrder()])\n\nConvert non-permuted (logical) global indices to local indices.\n\nIf order = MemoryOrder(), returned indices will be permuted using the permutation associated to the pencil configuration p.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Index","page":"Pencil configurations","title":"Index","text":"","category":"section"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"Pages = [\"Pencils.md\"]","category":"page"},{"location":"Pencils/","page":"Pencil configurations","title":"Pencil configurations","text":"[1]: Why would we want this? One application is to efficiently perform FFTs along y, which, under this permutation, would be the fastest dimension. This is used by the PencilFFTs package.","category":"page"},{"location":"MPITopology/#sec:mpi_topology","page":"MPI topology","title":"MPI topology","text":"","category":"section"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"The MPITopology type defines the MPI Cartesian topology of the decomposition. In other words, it contains information about the number of decomposed dimensions, and the number of processes in each of these dimensions.","category":"page"},{"location":"MPITopology/#Construction","page":"MPI topology","title":"Construction","text":"","category":"section"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"The main MPITopology constructor takes a MPI communicator and a tuple specifying the number of processes in each dimension. For instance, to distribute 12 MPI processes on a 3  4 grid:","category":"page"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"comm = MPI.COMM_WORLD  # we assume MPI.Comm_size(comm) == 12\npdims = (3, 4)\ntopology = MPITopology(comm, pdims)","category":"page"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"A convenience constructor is provided that automatically chooses a default pdims from the number of processes and from the dimension N of decomposition grid. For instance, for a two-dimensional decomposition:","category":"page"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"topology = MPITopology(comm, Val(2))","category":"page"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"Under the hood, this works by letting MPI_Dims_create choose the number of divisions along each dimension.","category":"page"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"At the lower level, MPITopology uses MPI_Cart_create to define a Cartesian MPI communicator. For more control, one can also create a Cartesian communicator using MPI.Cart_create, and pass that to MPITopology:","category":"page"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"dims = [3, 4]  # note: array, not tuple!\nperiods = zeros(Int, N)\nreorder = false\ncomm_cart = MPI.Cart_create(comm, dims, periods, reorder)\ntopology = MPITopology(comm_cart)","category":"page"},{"location":"MPITopology/#Types","page":"MPI topology","title":"Types","text":"","category":"section"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"MPITopology","category":"page"},{"location":"MPITopology/#PencilArrays.Pencils.MPITopologies.MPITopology","page":"MPI topology","title":"PencilArrays.Pencils.MPITopologies.MPITopology","text":"MPITopology{N}\n\nDescribes an N-dimensional Cartesian MPI decomposition topology.\n\n\n\nMPITopology(comm::MPI.Comm, pdims::Dims{N})\n\nCreate N-dimensional MPI topology information.\n\nThe pdims tuple specifies the number of MPI processes to put in every dimension of the topology. The product of its values must be equal to the number of processes in communicator comm.\n\nExample\n\nDivide 2D topology into 4×2 blocks:\n\ncomm = MPI.COMM_WORLD\n@assert MPI.Comm_size(comm) == 8\ntopology = MPITopology(comm, (4, 2))\n\n\n\nMPITopology(comm::MPI.Comm, Val(N))\n\nConvenient MPITopology constructor defining an N-dimensional decomposition of data among all MPI processes in communicator.\n\nThe number of divisions along each of the N dimensions is automatically determined by a call to MPI.Dims_create!.\n\nExample\n\nCreate 2D decomposition grid:\n\ncomm = MPI.COMM_WORLD\ntopology = MPITopology(comm, Val(2))\n\n\n\nMPITopology{N}(comm_cart::MPI.Comm)\n\nCreate topology information from MPI communicator with Cartesian topology (typically constructed using MPI.Cart_create). The topology must have dimension N.\n\nExample\n\nDivide 2D topology into 4×2 blocks:\n\ncomm = MPI.COMM_WORLD\n@assert MPI.Comm_size(comm) == 8\ndims = [4, 2]\nperiods = [0, 0]\nreorder = false\ncomm_cart = MPI.Cart_create(comm, pdims, periods, reorder)\ntopology = MPITopology{2}(comm_cart)\n\n\n\n\n\n","category":"type"},{"location":"MPITopology/#Methods","page":"MPI topology","title":"Methods","text":"","category":"section"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"get_comm(::MPITopology)\ncoords_local(::MPITopology)\nlength(::MPITopology)\nndims(::MPITopology)\nsize(::MPITopology)","category":"page"},{"location":"MPITopology/#PencilArrays.Pencils.MPITopologies.get_comm-Tuple{MPITopology}","page":"MPI topology","title":"PencilArrays.Pencils.MPITopologies.get_comm","text":"get_comm(t::MPITopology)\n\nGet MPI communicator associated to an MPI Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"MPITopology/#PencilArrays.Pencils.MPITopologies.coords_local-Tuple{MPITopology}","page":"MPI topology","title":"PencilArrays.Pencils.MPITopologies.coords_local","text":"coords_local(t::MPITopology)\n\nGet coordinates of local process in MPI topology.\n\n\n\n\n\n","category":"method"},{"location":"MPITopology/#Base.length-Tuple{MPITopology}","page":"MPI topology","title":"Base.length","text":"length(t::MPITopology)\n\nGet total size of Cartesian topology (i.e. total number of MPI processes).\n\n\n\n\n\n","category":"method"},{"location":"MPITopology/#Base.ndims-Tuple{MPITopology}","page":"MPI topology","title":"Base.ndims","text":"ndims(p::Pencil)\n\nNumber of spatial dimensions associated to pencil data.\n\nThis corresponds to the total number of dimensions of the space, which includes the decomposed and non-decomposed dimensions.\n\n\n\n\n\nndims(t::MPITopology)\n\nGet dimensionality of Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"MPITopology/#Base.size-Tuple{MPITopology}","page":"MPI topology","title":"Base.size","text":"size(t::MPITopology)\n\nGet dimensions of Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"MPITopology/#Index","page":"MPI topology","title":"Index","text":"","category":"section"},{"location":"MPITopology/","page":"MPI topology","title":"MPI topology","text":"Pages = [\"MPITopology.md\"]","category":"page"},{"location":"PencilIO/#PencilIO_module","page":"Parallel I/O","title":"Parallel I/O","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"CurrentModule = PencilArrays.PencilIO","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"The PencilArrays.PencilIO module contains functions for saving and loading PencilArrays to disk using parallel I/O. Currently, two different output formats are supported:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"raw binary files via the MPI-IO interface;\nparallel HDF5 files.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"In both cases, information on dataset sizes, names and other metadata are included along with the binary data.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"The implemented approach consists in storing the data coming from different MPI processes in a single file. This strategy scales better in terms of number of files, and is more convenient, than that of storing one file per process. However, the performance is very sensitive to the configuration of the underlying file system. In distributed file systems such as Lustre, it is worth tuning parameters such as the stripe count and stripe size. For more information, see for instance the Parallel HDF5 page.","category":"page"},{"location":"PencilIO/#Getting-started","page":"Parallel I/O","title":"Getting started","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"The first step before writing PencilArrays is to choose the parallel I/O driver, which determines the format of the output data. Two different drivers are currently available:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"MPIIODriver: parallel I/O via the MPI-IO API and the MPI.jl wrappers. This driver writes a raw binary file, along with a JSON file describing dataset metadata (name, dimensions, location in file, ...);\nPHDF5Driver: parallel I/O via the Parallel HDF5 API and HDF5.jl. This driver requires a special set-up, as detailed in the dedicated section.","category":"page"},{"location":"PencilIO/#Writing-data","page":"Parallel I/O","title":"Writing data","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"To open a parallel file, pass the MPI communicator and an instance of the chosen driver to open. For instance, the following opens an MPI-IO file in write mode:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"ff = open(MPIIODriver(), \"filename.bin\", MPI.COMM_WORLD; write=true)","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Datasets, in the form of PencilArrays, can then be written as follows:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"v = PencilArray(...)\nff[\"velocity\"] = v","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"This writing step may be customised via keyword arguments such as chunks and collective. These options are supported by both MPI-IO and HDF5 drivers. For instance:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"ff[\"velocity\", chunks=true, collective=false] = v","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"See setindex! for the meaning of these options for each driver, as well as for driver-specific options.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"After datasets are written, the file should be closed as usual by doing close(ff). Note that the do-block syntax is also supported, as in","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"open(MPIIODriver(), \"filename.bin\", MPI.COMM_WORLD; write=true) do ff\n    ff[\"velocity\"] = v\nend","category":"page"},{"location":"PencilIO/#Reading-data","page":"Parallel I/O","title":"Reading data","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Data is loaded into an existent PencilArray using read!. For instance:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"v = PencilArray(...)\nff = open(MPIIODriver(), \"filename.bin\", MPI.COMM_WORLD; read=true)\n    read!(ff, v, \"velocity\")\nend","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Note that, for the MPI-IO driver, a filename.bin.json file must be present along with the filename.bin file containing the binary data. The JSON file is automatically generated when writing data with this driver.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Optional keyword arguments, such as collective, are also supported by read!.","category":"page"},{"location":"PencilIO/#setting_up_parallel_hdf5","page":"Parallel I/O","title":"Setting-up Parallel HDF5","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"If using the Parallel HDF5 driver, the HDF5.jl package must be available and configured with MPI support. Note that HDF5.jl versions previous to v0.15 are not supported.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Parallel HDF5 is not enabled in the default installation of HDF5.jl. For Parallel HDF5 to work, the HDF5 C libraries wrapped by HDF5.jl must be compiled with parallel support and linked to the specific MPI implementation that will be used for parallel I/O. HDF5.jl must be explicitly instructed to use parallel-enabled HDF5 libraries available in the system. Similarly, MPI.jl must be instructed to use the corresponding MPI libraries. This is detailed in the sections below.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Parallel-enabled HDF5 libraries are usually included in computing clusters and linked to the available MPI implementations. They are also available via the package manager of a number of Linux distributions. (For instance, Fedora includes the hdf5-mpich-devel and hdf5-openmpi-devel packages, respectively linked to the MPICH and OpenMPI libraries in the Fedora repositories.)","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"The following step-by-step guide assumes one already has access to parallel-enabled HDF5 libraries linked to an existent MPI installation.","category":"page"},{"location":"PencilIO/#.-Using-system-provided-MPI-libraries","page":"Parallel I/O","title":"1. Using system-provided MPI libraries","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Set the environment variable JULIA_MPI_BINARY=system and then run ]build MPI from Julia. For more control, one can also set the JULIA_MPI_PATH environment variable to the top-level installation directory of the MPI library.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"See the MPI.jl docs for details.","category":"page"},{"location":"PencilIO/#.-Using-parallel-HDF5-libraries","page":"Parallel I/O","title":"2. Using parallel HDF5 libraries","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Set the JULIA_HDF5_PATH environment variable to the top-level installation directory of the HDF5 libraries compiled with parallel support are found. Then run ]build HDF5 from Julia. Note that the selected HDF5 library must be linked to the MPI library chosen in the previous section. Also note that HDF5 library versions older than 0.10.4 are not supported by HDF5.jl. For the set-up to be persistent across HDF5.jl updates, consider setting JULIA_HDF5_PATH in ~/.bashrc or similar.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"See the HDF5.jl README for details.","category":"page"},{"location":"PencilIO/#.-Loading-PencilIO","page":"Parallel I/O","title":"3. Loading PencilIO","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"In the PencilIO module, the HDF5.jl package is lazy-loaded using Requires. This means that, in Julia code, PencilArrays must be loaded after HDF5 for parallel I/O functionality to be available.","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"The following order of usings ensures that parallel I/O support is available:","category":"page"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"using MPI\nusing HDF5\nusing PencilArrays","category":"page"},{"location":"PencilIO/#Library","page":"Parallel I/O","title":"Library","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"PencilIO.ParallelIODriver\nMPIIODriver\nPHDF5Driver\nPencilIO.MPIFile\nopen\nsetindex!\nread!\nhdf5_has_parallel","category":"page"},{"location":"PencilIO/#PencilArrays.PencilIO.ParallelIODriver","page":"Parallel I/O","title":"PencilArrays.PencilIO.ParallelIODriver","text":"ParallelIODriver\n\nAbstract type specifying a parallel I/O driver.\n\n\n\n\n\n","category":"type"},{"location":"PencilIO/#PencilArrays.PencilIO.MPIIODriver","page":"Parallel I/O","title":"PencilArrays.PencilIO.MPIIODriver","text":"MPIIODriver(; sequential = false, uniqueopen = false, deleteonclose = false)\n\nMPI-IO driver using the MPI.jl package.\n\nKeyword arguments are passed to MPI.File.open.\n\nThis driver writes binary data along with a JSON file containing metadata. When reading data, this JSON file is expected to be present along with the raw data file.\n\n\n\n\n\n","category":"type"},{"location":"PencilIO/#PencilArrays.PencilIO.PHDF5Driver","page":"Parallel I/O","title":"PencilArrays.PencilIO.PHDF5Driver","text":"PHDF5Driver(; fcpl, fapl)\n\nParallel HDF5 driver using the HDF5.jl package.\n\nHDF5 file creation and file access property lists may be specified via the fcpl and fapl keyword arguments respectively.\n\nNote that the MPIO file access property list does not need to be set, as this is done automatically by this driver when the file is opened.\n\n\n\n\n\n","category":"type"},{"location":"PencilIO/#PencilArrays.PencilIO.MPIFile","page":"Parallel I/O","title":"PencilArrays.PencilIO.MPIFile","text":"MPIFile\n\nWraps a MPI.FileHandle, also including file position information and metadata.\n\nFile position is updated when reading and writing data, and is independent of the individual and shared file pointers defined by MPI.\n\n\n\n\n\n","category":"type"},{"location":"PencilIO/#Base.open","page":"Parallel I/O","title":"Base.open","text":"open([f::Function], driver::ParallelIODriver, filename, comm::MPI.Comm; keywords...)\n\nOpen parallel file using the chosen driver.\n\nKeyword arguments\n\nSupported keyword arguments include:\n\nopen mode arguments: read, write, create, append and truncate. These have the same behaviour and defaults as Base.open. Some of them may be ignored by the chosen driver (see driver-specific docs).\nas in MPI.File.open, other arguments are passed via an MPI.Info object.\n\nNote that driver-specific options (such as HDF5 property lists) must be passed to each driver's constructor.\n\nSee also\n\nopen(::MPIIODriver) for MPI-IO specific options\nopen(::PHDF5Driver) for HDF5 specific options\n\n\n\n\n\nopen([f::Function], driver::MPIIODriver, filename, comm::MPI.Comm; keywords...)\n\nOpen parallel file using the MPI-IO driver.\n\nSee open(::ParallelIODriver) for common options for all drivers.\n\nDriver-specific options may be passed via the driver argument. See MPIIODriver for details.\n\nDriver notes\n\nthe truncate keyword is ignored.\n\n\n\n\n\nopen([f::Function], driver::PHDF5Driver, filename, comm::MPI.Comm; keywords...)\n\nOpen parallel file using the Parallel HDF5 driver.\n\nSee open(::ParallelIODriver) for common options for all drivers.\n\nDriver-specific options may be passed via the driver argument. See PHDF5Driver for details.\n\n\n\n\n\n","category":"function"},{"location":"PencilIO/#Base.setindex!","page":"Parallel I/O","title":"Base.setindex!","text":"setindex!(file::MPIFile, x::MaybePencilArrayCollection,\n          name::AbstractString; chunks = false, collective = true, infokws...)\n\nWrite PencilArray to binary file using MPI-IO.\n\nOptional arguments\n\nif chunks = true, data is written in contiguous blocks, with one block per process. Otherwise, each process writes to discontiguous sections of disk, using MPI.File.set_view! and custom datatypes. Note that discontiguous I/O (the default) is more convenient, as it allows to read back the data using a different number or distribution of MPI processes.\nif collective = true, the dataset is written collectivelly. This is usually recommended for performance.\nwhen writing discontiguous blocks, additional keyword arguments are passed via an MPI.Info object to MPI.File.set_view!. This is ignored if chunks = true.\n\n\n\n\n\nsetindex!(\n    g::Union{HDF5.File,HDF5.Group}, x::MaybePencilArrayCollection,\n    name::AbstractString; chunks = false, collective = true, prop_lists...,\n)\n\nWrite PencilArray or PencilArrayCollection to parallel HDF5 file.\n\nFor performance reasons, the memory layout of the data is conserved. In other words, if the dimensions of a PencilArray are permuted in memory, then the data is written in permuted form.\n\nIn the case of a PencilArrayCollection, each array of the collection is written as a single component of a higher-dimension dataset.\n\nOptional arguments\n\nif chunks = true, data is written in chunks, with roughly one chunk per MPI process. This may (or may not) improve performance in parallel filesystems.\nif collective = true, the dataset is written collectivelly. This is usually recommended for performance.\nadditional property lists may be specified by key-value pairs in prop_lists, following the HDF5.jl syntax. These property lists take precedence over keyword arguments. For instance, if the dxpl_mpio = HDF5.H5FD_MPIO_COLLECTIVE option is passed, then the value of the collective argument is ignored.\n\nProperty lists\n\nProperty lists are passed to h5d_create and h5d_write. The following property types are recognised:\n\nlink creation properties,\ndataset creation properties,\ndataset access properties,\ndataset transfer properties.\n\nExample\n\nOpen a parallel HDF5 file and write some PencilArrays to the file:\n\npencil = Pencil(#= ... =#)\nu = PencilArray{Float64}(undef, pencil)\nv = similar(u)\n\n# [fill the arrays with interesting values...]\n\ncomm = get_comm(u)\n\nopen(PHDF5Driver(), \"filename.h5\", comm, write=true) do ff\n    ff[\"u\", chunks=true] = u\n    ff[\"uv\"] = (u, v)  # this is a two-component PencilArrayCollection (assuming equal dimensions of `u` and `v`)\nend\n\n\n\n\n\n","category":"function"},{"location":"PencilIO/#Base.read!","page":"Parallel I/O","title":"Base.read!","text":"read!(file::MPIFile, x::PencilArray, name::AbstractString;\n      collective = true, infokws...)\n\nRead binary data from an MPI-IO stream, filling in PencilArray.\n\nSee setindex! for details on keyword arguments.\n\n\n\n\n\nread!(g::Union{HDF5File,HDF5Group}, x::MaybePencilArrayCollection,\n      name::AbstractString; collective=true, prop_lists...)\n\nRead PencilArray or PencilArrayCollection from parallel HDF5 file.\n\nSee setindex! for details on optional arguments.\n\nProperty lists\n\nProperty lists are passed to h5d_open and h5d_read. The following property types are recognised:\n\ndataset access properties,\ndataset transfer properties.\n\nExample\n\nOpen a parallel HDF5 file and read some PencilArrays:\n\npencil = Pencil(#= ... =#)\nu = PencilArray{Float64}(undef, pencil)\nv = similar(u)\n\ncomm = get_comm(u)\ninfo = MPI.Info()\n\nopen(PHDF5Driver(), \"filename.h5\", comm, read=true) do ff\n    read!(ff, u, \"u\")\n    read!(ff, (u, v), \"uv\")\nend\n\n\n\n\n\n","category":"function"},{"location":"PencilIO/#PencilArrays.PencilIO.hdf5_has_parallel","page":"Parallel I/O","title":"PencilArrays.PencilIO.hdf5_has_parallel","text":"hdf5_has_parallel() -> Bool\n\nReturns true if the loaded HDF5 libraries support MPI-IO.\n\n\n\n\n\n","category":"function"},{"location":"PencilIO/#Index","page":"Parallel I/O","title":"Index","text":"","category":"section"},{"location":"PencilIO/","page":"Parallel I/O","title":"Parallel I/O","text":"Pages = [\"PencilIO.md\"]","category":"page"},{"location":"Transpositions/#Global-MPI-operations","page":"Global MPI operations","title":"Global MPI operations","text":"","category":"section"},{"location":"Transpositions/","page":"Global MPI operations","title":"Global MPI operations","text":"CurrentModule = PencilArrays","category":"page"},{"location":"Transpositions/","page":"Global MPI operations","title":"Global MPI operations","text":"One of the most time-consuming parts of a large-scale computation involving multidimensional FFTs, is the global data transpositions between different MPI decomposition configurations. In PencilArrays, this is performed by the transpose! function, which takes two PencilArrays, typically associated to two different configurations. The implementation performs comparably to similar implementations in lower-level languages (see PencilFFTs benchmarks for details).","category":"page"},{"location":"Transpositions/","page":"Global MPI operations","title":"Global MPI operations","text":"Also provided is a gather function that creates a single global array from decomposed data. This can be useful for tests (in fact, it is used in the PencilArrays tests to verify the correctness of the transpositions), but shouldn't be used with large datasets. It is generally useful for small problems where the global size of the data can easily fit the locally available memory.","category":"page"},{"location":"Transpositions/#Library","page":"Global MPI operations","title":"Library","text":"","category":"section"},{"location":"Transpositions/","page":"Global MPI operations","title":"Global MPI operations","text":"Transpositions.Transposition\nTranspositions.transpose!\nMPI.Waitall!\ngather","category":"page"},{"location":"Transpositions/#PencilArrays.Transpositions.Transposition","page":"Global MPI operations","title":"PencilArrays.Transpositions.Transposition","text":"Transposition\n\nHolds data for transposition between two pencil configurations.\n\n\n\nTransposition(dest::PencilArray{T,N}, src::PencilArray{T,N};\n              method = Transpositions.PointToPoint())\n\nPrepare transposition of arrays from one pencil configuration to the other.\n\nThe two pencil configurations must be compatible for transposition:\n\nthey must share the same MPI Cartesian topology,\nthey must have the same global data size,\nwhen written as a sorted tuple, the decomposed dimensions must be almost the same, with at most one difference. For instance, if the input of a 3D dataset is decomposed in (2, 3), then the output may be decomposed in (1, 3), but not in (1, 2). If the decomposed dimensions are the same, then no transposition is performed, and data is just copied if needed.\n\nThe src and dest arrays may be aliased (they can share memory space).\n\nPerformance tuning\n\nThe method argument allows to choose between transposition implementations. This can be useful to tune performance of MPI data transfers. Two values are currently accepted:\n\nTranspositions.PointToPoint() uses non-blocking point-to-point data transfers (MPI_Isend and MPI_Irecv). This may be more performant since data transfers are interleaved with local data transpositions (index permutation of received data). This is the default.\nTranspositions.Alltoallv() uses collective MPI_Alltoallv for global data transpositions.\n\n\n\n\n\n","category":"type"},{"location":"Transpositions/#LinearAlgebra.transpose!","page":"Global MPI operations","title":"LinearAlgebra.transpose!","text":"transpose!(t::Transposition; waitall=true)\ntranspose!(dest::PencilArray{T,N}, src::PencilArray{T,N};\n           method = Transpositions.PointToPoint())\n\nTranspose data from one pencil configuration to the other.\n\nThe first variant allows to optionally delay the wait for MPI send operations to complete. This is useful if the caller wants to perform other operations with the already received data. To do this, the caller should pass waitall=false, and manually invoke MPI.Waitall! on the Transposition object once the operations are done. Note that this option only has an effect when the transposition method is PointToPoint.\n\nSee Transposition for details.\n\n\n\n\n\n","category":"function"},{"location":"Transpositions/#MPI.Waitall!","page":"Global MPI operations","title":"MPI.Waitall!","text":"MPI.Waitall!(t::Transposition)\n\nWait for completion of all unfinished MPI communications related to the transposition.\n\n\n\n\n\n","category":"function"},{"location":"Transpositions/#PencilArrays.gather","page":"Global MPI operations","title":"PencilArrays.gather","text":"gather(x::PencilArray, [root::Integer=0])\n\nGather data from all MPI processes into one (big) array.\n\nData is received by the root process.\n\nReturns the full array on the root process, and nothing on the other processes.\n\nThis can be useful for testing, but it shouldn't be used with very large datasets!\n\n\n\n\n\n","category":"function"},{"location":"Transpositions/#Index","page":"Global MPI operations","title":"Index","text":"","category":"section"},{"location":"Transpositions/","page":"Global MPI operations","title":"Global MPI operations","text":"Pages = [\"Transpositions.md\"]","category":"page"},{"location":"PencilArrays_timers/#PencilArrays.measuring_performance","page":"Measuring performance","title":"Measuring performance","text":"","category":"section"},{"location":"PencilArrays_timers/","page":"Measuring performance","title":"Measuring performance","text":"It is possible to measure the time spent in different sections of the MPI data transposition routines using the TimerOutputs package. This has a (very small) performance overhead, so it is disabled by default. To enable time measurements, call TimerOutputs.enable_debug_timings after loading PencilArrays (see below for an example). For more details see the TimerOutputs docs.","category":"page"},{"location":"PencilArrays_timers/","page":"Measuring performance","title":"Measuring performance","text":"Minimal example:","category":"page"},{"location":"PencilArrays_timers/","page":"Measuring performance","title":"Measuring performance","text":"using MPI\nusing PencilArrays\nusing TimerOutputs\n\n# Enable timing of `PencilArrays` functions\nTimerOutputs.enable_debug_timings(PencilArrays)\nTimerOutputs.enable_debug_timings(Transpositions)\n\nMPI.Init()\n\npencil = Pencil(#= args... =#)\n\n# [do stuff with `pencil`...]\n\n# Retrieve and print timing data associated to `plan`\nto = timer(pencil)\nprint_timer(to)","category":"page"},{"location":"PencilArrays_timers/","page":"Measuring performance","title":"Measuring performance","text":"By default, each Pencil has its own TimerOutput. If you already have a TimerOutput, you can pass it to the Pencil constructor:","category":"page"},{"location":"PencilArrays_timers/","page":"Measuring performance","title":"Measuring performance","text":"to = TimerOutput()\npencil = Pencil(..., timer=to)\n\n# [do stuff with `pencil`...]\n\nprint_timer(to)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = PencilArrays","category":"page"},{"location":"#PencilArrays","page":"Home","title":"PencilArrays","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Distributed Julia arrays using the MPI protocol.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a convenient framework for working with multidimensional Julia arrays distributed among MPI processes.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The name of this package originates from the decomposition of 3D domains along two out of three dimensions, sometimes called pencil decomposition. This is illustrated by the figure below,[1] where each coloured block is managed by a different MPI process.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"figure\">\n  <img\n    width=\"85%\"\n    src=\"img/pencils.svg\"\n    alt=\"Pencil decomposition of 3D domains\">\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"More generally, PencilArrays can decompose arrays of arbitrary dimension N, along an arbitrary subset of M dimensions. (In the example above, N = 3 and M = 2.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"PencilArrays is the basis for the PencilFFTs package, which provides efficient and highly scalable distributed FFTs.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"distribution of N-dimensional arrays among MPI processes;\ndecomposition of arrays along an arbitrary subset of dimensions;\ntranspositions between different decomposition configurations, using point-to-point and collective MPI communications;\nzero-cost, convenient dimension permutations using StaticPermutations;\nconvenient parallel I/O using either MPI-IO or the Parallel HDF5 libraries;\ndistributed FFTs and related transforms via the PencilFFTs package.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PencilArrays can be installed using the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ] add PencilArrays","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following example assumes that the code is executed on 12 MPI processes. The processes are distributed on a 34 grid, as in the figure above.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MPI\nusing PencilArrays\nusing LinearAlgebra: transpose!\n\nMPI.Init()\ncomm = MPI.COMM_WORLD       # we assume MPI.Comm_size(comm) == 12\nrank = MPI.Comm_rank(comm)  # rank of local process, in 0:11\n\n# Define MPI Cartesian topology: distribute processes on a 3×4 grid.\ntopology = MPITopology(comm, (3, 4))\n\n# Let's decompose 3D arrays along dimensions (2, 3).\n# This corresponds to the \"x-pencil\" configuration in the figure.\n# This configuration is described by a Pencil object.\ndims_global = (42, 31, 29)  # global dimensions of the array\ndecomp_dims = (2, 3)\npen_x = Pencil(topology, dims_global, decomp_dims)\n\n# We can now allocate distributed arrays in the x-pencil configuration.\nAx = PencilArray{Float64}(undef, pen_x)\nfill!(Ax, rank * π)  # each process locally fills its part of the array\nparent(Ax)           # parent array (here, an Array{Float64,3}) holding the local data\nsize(Ax)             # size of local part\nsize_global(Ax)      # total size of the array = (42, 31, 29)\n\n# Create another pencil configuration, decomposing along dimensions (1, 3).\n# We could use the same constructor as before, but it's recommended to reuse the\n# previous Pencil instead.\npen_y = Pencil(pen_x, decomp_dims=(1, 3))\n\n# Now transpose from the x-pencil to the y-pencil configuration, redistributing\n# the data initially in Ax.\nAy = PencilArray{Float64}(undef, pen_y)\ntranspose!(Ay, Ax)","category":"page"},{"location":"","page":"Home","title":"Home","text":"[1]: Figure adapted from this PhD thesis.","category":"page"}]
}
